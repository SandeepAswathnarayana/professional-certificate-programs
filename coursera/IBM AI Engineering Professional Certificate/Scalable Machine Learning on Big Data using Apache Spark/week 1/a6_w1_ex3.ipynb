{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Welcome to exercise three of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll create a DataFrame, register a temporary query table and issue SQL commands against it. \n\nLet\u2019s create a little data frame:"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import Row\n\ndf = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])\n\n# let's have a look what's inside\ndf.show()\n\n# let's print the schema\ndf.printSchema()", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200302134615-0000\nKERNEL_ID = aec058b3-7ca8-4bac-9415-b8a77d6a97c3\n+---+------+\n| id| value|\n+---+------+\n|  1|value1|\n|  2|value2|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we register this DataFrame as query table and issue an SQL statement against it. Please note that the result of the SQL execution returns a new DataFrame we can work with."}, {"metadata": {}, "cell_type": "code", "source": "# register dataframe as query table\ndf.createOrReplaceTempView('df_view')\n\n# execute SQL query\ndf_result = spark.sql('select value from df_view where id=2')\n\n#\u00a0examine contents of result\ndf_result.show()\n\n# get result as string\ndf_result.first().value", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "+------+\n| value|\n+------+\n|value2|\n+------+\n\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "'value2'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Although we\u2019ll learn more about DataFrames next week, please try to find a way to count the rows in this DataFrame by looking at the API documentation. No worries, we\u2019ll cover DataFrames in more detail next week.\n\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"}, {"metadata": {}, "cell_type": "code", "source": "df.count()", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "2"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}